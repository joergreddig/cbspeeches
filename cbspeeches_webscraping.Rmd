---
title: "Web scraping"
author: "Joerg Reddig"
date: "15 August 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

##--------- Loading packages ----------##

library(tidyverse)
#library(stringr)
library(rvest)
library(V8)



```



## Web scraping for cbspeeches


### Resources: 

* Webinar with Hadley Wickham on webscraping: <https://www.youtube.com/watch?v=tHszX31_r4s>
* webscraping with R: https://sites.google.com/a/stanford.edu/rcpedia/screen-scraping/web-scraping-with-r
* rvest tutorial: https://stat4701.github.io/edav/2015/04/02/rvest_tutorial/


### Selector Gadget

Selector gadget (from <www.selectorgadget.com>) helps to easily identify the selector for specific xml feeds from websites. 



### rvest package

* tutorial video <https://www.youtube.com/watch?v=82s8KdZt5v8>, <https://www.youtube.com/watch?v=f_FmrzOzfNg>


https://stat4701.github.io/edav/2015/04/02/rvest_tutorial/
https://stackoverflow.com/questions/39246739/downloading-multiple-files-using-download-file-function-in-r



------------------------------------------------

## ECB speeches

* YT tutorial: https://www.youtube.com/watch?v=82s8KdZt5v8
* Datacamp tutorial: https://www.datacamp.com/community/tutorials/r-web-scraping-rvest


### Definition of functions

```{r}
ecb_url <- "https://www.ecb.europa.eu/press/key/date/%d/html/index.en.html"


## function to retrieve list of the links to ecb speeches
get_speech_list_ecb <- function (i) {
    
    # concatenate the ecb url with the iterator
    page <- read_html(sprintf(ecb_url, i)) 

    # create vector of all speech links
    speech_links <- tibble(
        #links = html_attr(html_nodes(page, ".doc-subtitle+ .ecb-langSelector a"), "href"),
        ###links = html_attr(html_nodes(page, "#ecb-content-col .arrow"), "href"),
        links = html_attr(html_nodes(page, ".offeredLanguage a"), "href"),
        date  = html_text(html_nodes(page, "dt"))
    )
}

## function to retrieve list of the links to ecb speeches
## 2007 doesn't work properly, needs own functions
get_speech_list_ecb_2007 <- function (i) {
    
    # concatenate the ecb url with the iterator
    page <- read_html(sprintf(ecb_url, i)) 

    
    # get links and dates individually
    only_links <-  html_attr(html_nodes(page, "#ecb-content-col .arrow"), "href")
    
    only_dates <- html_text(html_nodes(page, "dt"))
    
    
    # element after which a link is missing
    indexed_element  <- "/press/key/date/2007/html/sp070213.en.html" #wenn link, dann dahinter:

    # missing link to be inserted after the index element
    insert_element <- "/press/key/date/2007/html/sp070205.de.html" #einfügen

    # use match() to find the index of the indexed element in the only_links vector
    index <- match(indexed_element, only_links)

    # append the only_links vector with the missing insert_element after the indexed_element
    # append(x, values, after = length(x))
    only_links <- append(only_links, insert_element, after = index)

    
    # create vector of all speech links
    speech_links <- tibble(
        links = only_links,
        date = only_dates
        )

    return(speech_links)
}


## function to retrieve list of the links to ecb speeches
get_speech_list_ecb_2019 <- function (i) {
    
    # concatenate the ecb url with the iterator
    page <- read_html(sprintf(ecb_url, i)) 

    # create vector of all speech links
    speech_links <- tibble(
        links = html_attr(html_nodes(page, "#ecb-content-col .arrow"), "href"),
        date  = html_text(html_nodes(page, "dt"))
    )
}


###

ecb_homepage <- "https://www.ecb.europa.eu"


# function for storing the speech texts in a data frame 
scrape_ecb <- function (i) {
    # blablabla
    page <- read_html(paste0(ecb_homepage, ecb_links[i, ]))

    # create data frame for storing the speech texts and metadata
    ecb_speeches <- tibble(
        date     = html_text(html_nodes(page, ".ecb-bcCurrent")),
        title    = html_text(html_nodes(page, ".ecb-pressContentTitle")),
        subtitle = html_text(html_nodes(page, ".ecb-pressContentSubtitle")),
        text     = paste(html_text(html_nodes(page, "#ecb-content-col li , article p")), collapse = " "), ###instead of only article p
        footnote = paste(html_text(html_nodes(page, ".footnote")), collapse = " ")
    )    
}


# for 2006:

scrape_ecb_2006 <- function (i) {
    # blablabla
    page <- read_html(paste0(ecb_homepage, ecb_links[i, 1]))

    # create data frame for storing the speech texts and metadata
    ecb_speeches <- tibble(
        #date     = html_text(html_nodes(page, ".ecb-bcCurrent")),
        #date     = ecb_links[, 2],
        title    = html_text(html_nodes(page, ".ecb-pressContentTitle")),
        subtitle = html_text(html_nodes(page, ".ecb-pressContentPubDate")),
        text     = paste(html_text(html_nodes(page, "article p")), collapse = " "), ###instead of only article p
        footnote = paste(html_text(html_nodes(page, ".footnote p")), collapse = " ")
    )    
}


# for 2019 --doesnt work so far!!!
scrape_ecb_x <- function (i) {
    # blablabla
    page <- read_html(paste0(ecb_homepage, ecb_links[i, 1]))

    # create data frame for storing the speech texts and metadata
    ecb_speeches <- tibble(
        #date     = html_text(html_nodes(page, ".ecb-bcCurrent")),
        title    = html_text(html_nodes(page, ".ecb-pressContentTitle")),
        subtitle = html_text(html_nodes(page, ".ecb-pressContentSubtitle")),
        text     = paste(html_text(html_nodes(page, "#ecb-content-col li , article p")), collapse = " "), ###instead of only article p
        footnote = paste(html_text(html_nodes(page, ".footnote")), collapse = " ")
    )    
}

```


### Actual scraping for ECB speeches

```{r}

# get list of links to speeches 2000-2011 without 2004, 2007, and 2010
ecb_links <- map_df(2003, get_speech_list_ecb) # c(2000:2003, 2005, 2006, 2008, 2009, 2011:2019)

# for 2007 get list of links: 
ecb_links <- map_df(2007, get_speech_list_ecb_2007)

ecb_links <- map_df(2019, get_speech_list_ecb_2019) %>% 
    # link of speech on 29.3.2019 is not recognised correctly:
    mutate(links = replace(links, links == "/pub/annual/html/ar2018~d08cb4c623.en.html", "/press/key/date/2019/html/ecb.sp190329~da3110cea9.en.html"))



#### need to get rid of non-english speeches!

ecb_speeches <- map_df(1:nrow(ecb_links), scrape_ecb_x) 

ecb_speeches <- map_df(1:nrow(ecb_links), scrape_ecb_2006) # 2006, 2007


##--------------------------------------------------------------

# add date column to speeches vector
ecb_speeches <- bind_cols(ecb_links[, 2], ecb_speeches)

# transform it to prober date column
ecb_speeches <- ecb_speeches %>% 
    mutate(date = as.Date(date, format = "%d/%m/%Y"))




##--------------------------------------------------------------
## Add speaker names ##

for (i in 1:nrow(ecb_speeches)) {
    if(str_detect(ecb_speeches$subtitle[i], "Jean-Claude Trichet")) {
        ecb_speeches$speaker[i] <- "Jean-Claude Trichet"
    } else if (str_detect(ecb_speeches$subtitle[i], "The publication of the translation was authorised by Le Monde")) {
        ecb_speeches$speaker[i] <- "Jean-Claude Trichet"
    } else if (str_detect(ecb_speeches$subtitle[i], "Euro Vision")) {
        ecb_speeches$speaker[i] <- "Jean-Claude Trichet"
    } else if (str_detect(ecb_speeches$subtitle[i], "Economic and Monetary Affairs Committee")) {
        ecb_speeches$speaker[i] <- "Jean-Claude Trichet"
    } else if (str_detect(ecb_speeches$subtitle[i], "Jürgen Stark")) {
        ecb_speeches$speaker[i] <- "Juergen Stark"
    } else if (str_detect(ecb_speeches$subtitle[i], "Gertrude Tumpel-Gugerell")) {
        ecb_speeches$speaker[i] <- "Gertrude Tumpel-Gugerell"
    } else if (str_detect(ecb_speeches$subtitle[i], "José Manuel González-Páramo")) {
        ecb_speeches$speaker[i] <- "Jose Manuel Gonzalez-Paramo"
    } else if (str_detect(ecb_speeches$subtitle[i], "Papademos")) {
        ecb_speeches$speaker[i] <- "Lucas Papademos"
    } else if (str_detect(ecb_speeches$subtitle[i], "Lorenzo Bini Smaghi")) {
        ecb_speeches$speaker[i] <- "Lorenzo Bini Smaghi"
    } else if (str_detect(ecb_speeches$subtitle[i], "Otmar Issing")) {
        ecb_speeches$speaker[i] <- "Otmar Issing"
    } else if (str_detect(ecb_speeches$subtitle[i], "Mario Draghi")) {
        ecb_speeches$speaker[i] <- "Mario Draghi"
    } else if (str_detect(ecb_speeches$subtitle[i], "Vítor Constâncio")) {
        ecb_speeches$speaker[i] <- "Vitor Constancio"
    } else if (str_detect(ecb_speeches$subtitle[i], "Duisenberg")) {
        ecb_speeches$speaker[i] <- "Willem Duisenberg" 
    } else if (str_detect(ecb_speeches$subtitle[i], "Ottmar Issing")) {
        ecb_speeches$speaker[i] <- "Ottmar Issing" 
    } else if (str_detect(ecb_speeches$subtitle[i], "Padoa-Schioppa")) {
        ecb_speeches$speaker[i] <- "Tommaso Padoa-Schioppa" 
    } else if (str_detect(ecb_speeches$subtitle[i], "Noyer")) {
        ecb_speeches$speaker[i] <- "Christian Noyer" 
    } else if (str_detect(ecb_speeches$subtitle[i], "Eugenio Domingo Solans")) {
        ecb_speeches$speaker[i] <- "Eugenio Domingo Solans" 
    } else if (str_detect(ecb_speeches$subtitle[i], "Sirkka Hämäläinen")) {
        ecb_speeches$speaker[i] <- "Sirkka Haemaelaeinen" 
    } else if (str_detect(ecb_speeches$subtitle[i], "Peter Praet")) {
        ecb_speeches$speaker[i] <- "Peter Praet"
    } else if (str_detect(ecb_speeches$subtitle[i], "Benoît Cœuré")) {
        ecb_speeches$speaker[i] <- "Benoit Coeure"
    } else if (str_detect(ecb_speeches$subtitle[i], "Yves Mersch")) {
        ecb_speeches$speaker[i] <- "Yves Mersch"
    } else if (str_detect(ecb_speeches$subtitle[i], "Sabine Lautenschläger")) {
        ecb_speeches$speaker[i] <- "Sabine Lautenschlaeger"
    } else if (str_detect(ecb_speeches$subtitle[i], "Luis de Guindos")) {
        ecb_speeches$speaker[i] <- "Luis de Guindos"
    } else {
        ecb_speeches$speaker[i] <- "unknown"
    } 
}
 
 
 
 
# unknown <- ecb_speeches %>% filter(speaker == "unknown") %>% select(speaker, subtitle)

    
#------------to do---------------#
# extract speaker
# create id for each speech
# extract footnotes if available


```


## Create "All Speeches"-type table

```{r}
ecb_speeches %>% 
    mutate(institution = "ECB", 
           doc_id = paste0(date, " - ", institution, " - ", speaker, ".txt"),
           city = "",
           country = "", 
           length = "", 
           words_per_sentence = "", 
           characters_per_word = "", 
           flesch = "", 
           flesch_grade = "", 
           language = "", 
           source = "") %>%
    select(doc_id, date, speaker, city, country, subtitle, title, length, words_per_sentence, characters_per_word, flesch, flesch_grade, language, institution, source) %>% 
    write_csv2("ecb_speeches_scraped.csv")
```



## Save scraped texts to files

```{r save2file}


save_speeches_to_file <- function(index){
    #write each speech to a text document
    write(ecb_speeches$text[index], file = paste0(ecb_speeches$date[index], " - ECB - ", ecb_speeches$speaker[index], ".txt"))
}
 

# need to write with walk function as saving file is a side effect
walk(1:nrow(ecb_speeches), save_speeches_to_file)


```





```{r}
# scrape_ecb_overview <- function (i) {
#     
#     # concatenate the ecb url with the iterator
#     page <- read_html(sprintf(ecb_url, i)) 
#     
#     # create data frame with info about the speeches
#     speeches_df <- data.frame(
#         date  = html_text(html_nodes(page, "dt")), 
#         title = html_text(html_nodes(page, "dd > .doc-title a")), 
#         info  = html_text(html_nodes(page, ".doc-subtitle")), 
#         language  = html_text(html_nodes(page, "dd > .ecb-langSelector .offeredLanguage")),
#         #link  =  html_attr(html_nodes(page, ".doc-subtitle+ .ecb-langSelector a"), "href"), 
#         # doesnt work to select the links for all identified speeches, pdfs should be included
#         stringsAsFactors = FALSE
#     )
# 
#     
#     # return the data frame    
#     return(speeches_df)
# }
# 
# ecb_info <- map_df(2018, scrape_ecb)

```




### Text cleaning

* get rid of footnote marks
* what about the line breaks? How can they be kept?




---------------------------------

## FED speeches


```{r}
fed_url <- "https://www.federalreserve.gov/newsevents/speech/%dspeech.htm"
fedtest <- "https://www.federalreserve.gov/newsevents/speech/2010speech.htm"


# works also with javascript
newfedurl <- "https://www.federalreserve.gov/newsevents/speeches.htm"
page <- read_html(newfedurl)




## function to retrieve list of the fed speech links and metadata
get_speech_list_fed <- function (i) {
    
    # concatenate the ecb url with the iterator
    page <- read_html(sprintf(fed_url, i)) 
        
    ## only valid for speeches from 1996 -- 2005
    if (i < 2006) {

        # create vector of all speech links
        speech_links <- tibble(
            date      = page %>% html_nodes("#speechIndex li") %>% html_text(),
            speaker   = page %>% html_nodes(".speaker") %>% html_text(),
            title     = page %>% html_nodes("#speechIndex a") %>% html_text(),
            subtitle  = page %>% html_nodes(".location") %>% html_text(),
            link      = page %>% html_nodes("#speechIndex a") %>% html_attr("href")
        ) %>% 
        mutate(date = str_sub(date, start = 1, end = 25)) # get the date right with stringr!!!
    } else if (i > 2005 & i <= 2010) {
    # only valid for speeches as of 2006 to 2010
        ## prossibly one can use the new page until 2018 for all speeches after 2005, so this part 
        ##needs to be overhauled!
        
        # create vector of all speech links
        speech_links <- tibble(
            date      = page %>% html_nodes("time") %>% html_text(),
            speaker   = page %>% html_nodes(".news__speaker") %>% html_text(),
            title     = page %>% html_nodes("#content a em") %>% html_text(),
            subtitle  = page %>% html_nodes(".news__speaker+ p") %>% html_text(),
            link      = page %>% html_nodes("#content a em") %>% html_attr("href") ## der link klappt nicht
        ) 
    } else {
        print("blubsch!")
    }
}

# get list of links to speeches
#ecb_links <- map_df(2017:2018, get_speech_list_ecb)

#### need to get rid of non-english speeches!



#ecb_homepage <- "https://www.ecb.europa.eu"


# function for storing the speech texts in a data frame 
scrape_ecb <- function (i) {
    # blablabla
    page <- read_html(paste0(ecb_homepage, ecb_links[i, ]))

    # create data frame for storing the speech texts and metadata
    ecb_speeches <- tibble(
        date     = html_text(html_nodes(page, ".ecb-bcCurrent")),
        title    = html_text(html_nodes(page, ".ecb-pressContentTitle")),
        subtitle = html_text(html_nodes(page, ".ecb-pressContentSubtitle")),
        text     = paste(html_text(html_nodes(page, "article p")), collapse = "")
    )    

}

ecb_speeches <- map_df(1:100, scrape_ecb) 


```




---------------------------------------

## BoE speeches

Resources: 

PhantomJS: 

* https://www.datacamp.com/community/tutorials/scraping-javascript-generated-data-with-r#comments
* http://www.rladiesnyc.org/post/scraping-javascript-websites-in-r/




```{r}

boe_url <- "https://www.bankofengland.co.uk/speech/speeches"

page_js <- read_html(boe_url)

emailjs <- read_html(link) %>% html_nodes('li')# %>% html_nodes('script')# %>% html_text()


# scrape_boe <- function () {
#     # blablabla
#     page <- read_html(boe_url)
# 
#     # create data frame for storing the speech texts and metadata
#     boe_speeches <- tibble(
#         date     = html_text(html_nodes(page, ".release-date")),
#         title    = html_text(html_nodes(page, "#SearchResults .exclude-navigation")),
#         links    = html_attr(html_nodes(page, "#SearchResults .exclude-navigation"), "href")
#     )    
# 
# }

# ecb_speeches <- map_df(1:100, scrape_ecb) 

boe <- scrape_boe()



# download the actual pdf file to a folder
speech_url <- "https://www.bankofengland.co.uk/-/media/boe/files/speech/2018/good-cop-bad-cop-speech-by-sam-woods.pdf"

download.file(speech_url, "D:/Test.pdf", mode="wb")

# file should be named with the ID tag like the other speeches

```


```{r test_js_scraping}

#Loading both the required libraries
library(rvest)
library(V8)

#URL with js-rendered content to be scraped
link <- 'https://food.list.co.uk/place/22191-brewhemia-edinburgh/'

#Read the html page content and extract all javascript codes that are inside a list
emailjs <- read_html(link) %>% html_nodes('li') %>% html_nodes('script') %>% html_text()

# Create a new v8 context
ct <- v8()

#parse the html content from the js output and print it as text
read_html(ct$eval(gsub('document.write','',emailjs))) %>% 
 html_text()

```



```{r Rselenium_scraping}

library(RSelenium)
library(rvest)

# initialize browser and driver with RSelenium
ptm <- phantom()
rd <- remoteDriver(browserName = 'phantomjs')
rd$open()

# grab source for page
rd$navigate('https://fantasy.premierleague.com/a/entry/767830/history')
html <- rd$getPageSource()[[1]]

# clean up
rd$close()
ptm$stop()

# parse with rvest
df <- html %>% read_html() %>% 
    html_node('#ismr-event-history table.ism-table') %>% 
    html_table() %>% 
    setNames(gsub('\\S+\\s+(\\S+)', '\\1', names(.))) %>%    # clean column names
    setNames(gsub('\\s', '_', names(.)))

str(df)


```


------------------------------

## Save texts to files 

* text together with Footnotes saved as .txt
* one file per text
* naming linke ID


```{r}
print("clean text!")

```


