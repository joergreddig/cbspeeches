Let me welcome you to the 9th biennial conference organised by the Irving Fisher Committee. We are very glad to welcome speakers and participants from IFC central banks and from international organisations. I would especially like to welcome the authors of the seven papers which are being considered for this year’s "Young Statistician Award". A panel of experts will choose a winner who will be announced towards the end of the conference.
Ten years have elapsed since the financial crisis. Central banks statistics have contributed significantly to our understanding of the financial system, systemic risks, and potential vulnerabilities. Against this backdrop, this conference asks: Are post-crisis statistical initiatives completed?
Statistical initiatives are not an end in themselves: They need to address demand for information among the general public, academics, and policymakers. They need to balance the costs and benefits of collecting information. And they need to make best use of the available technology. Before going into these issues, let me give a quick refresher on post-crisis statistical initiatives.
The global financial crisis highlighted significant gaps in data needed to understand financial stability risks and implications for the real economy. Post-crisis reforms under the G20 Data Gaps Initiative (DGI) thus focused on: the assessment of international and cross-sector relationships, the monitoring of risks in the financial sector, better communication of official statistics.
The first phase of the DGI in the years 2009-15 focused on conceptual work. Key deliverables include revised Financial Soundness Indicators, the FSB’s annual Global Shadow Banking Monitoring Report, and a standardised data template for the collection of data on global systemically important banks (G-SIBs).
The second phase, which started in 2015, is focusing on implementation and data collection. Emphasis is being placed on monitoring of risk in the financial sector, vulnerabilities, interconnections and spillovers.
Linkages and contagion effects
As regards understanding cross-border and cross-sector linkages, we have made progress in terms of analysing contagion effects. The first three sessions of the conference will address contagion effects and their measurement through reporting from multinational enterprises, identification of international exposures and international flows of funds, and managing cross-border impacts on financial accounts.
The IFC has highlighted the usefulness of nationality-based indicators. Nationality-based indicators complement residency-based statistics. For example, they help understand who takes the underlying economic decisions, who is exposed to final risks, and in which countries the ultimately responsible unit resides. This work was conducted in cooperation with international organisations, in particular the OECD. 
Granular data and information systems
Shocks affecting individual banks or firms can have significant aggregate effects (Gabaix 2011, Amiti and Weinstein 2018). Yet, the collection of sufficiently granular statistics on individual banks and firms can be difficult and costly. Although a lot of granular information is potentially available, using and merging different datasets has been complicated by a lack of harmonised identifiers or the existence of legal constraints.
In 2017, the IFC and the European Committee of Central Balance Sheet Data Offices (ECCBSO) published a report which highlights the usefulness of firms’ individual financial statements (IFC 2017a). The report shows that the coverage of firm-level balance sheet information has improved. Yet, comparing data across different jurisdictions, and matching balance sheet data with other statistics, remains challenging.
The report also indicates that the use of granular financial data is often constrained by confidentiality considerations. It is important for us to improve information systems. The International Network for Exchanging Experience on Statistical Handling of Granular Data (INEXDA) is one example of such information sharing.
One area where granular data are particularly relevant is the surveillance of risks to financial stability. By their very nature, financial stability concerns arise whenever risks to individual institutions or sectors threaten the stability of the financial system as a whole. In 2017, the IFC therefore co-organised a workshop with the National Bank of Belgium to share views on data collection for macroprudential analysis. 
Understanding how shocks propagate within the financial system requires entity-level information on interlinkages and spillovers. This information is needed to monitor risks and to assess the effects of post-crisis financial sector reforms. Important data gaps, however, are still a major obstacle to the effective assessment of the impact of post-crisis reforms (ESRB 2016, Eurostat 2017, and OECD 2017). Among them are the measurement of prices in the real estate sector, the assessment of household vulnerabilities, and new developments such as related to FinTech.
Collecting data is not, and should not be, an end in itself. We need to join the dots, not just collect them. Data provide information about the structure and state of the financial system, and they can be used to assess the effects of post-crisis financial sector reforms.
After the crisis, several major reform initiatives were launched, which aim at enhancing the stability and resilience of the global financial system. So far, national and international bodies have focused on monitoring the implementation of these reforms (FSB2016). To answer the question of how financial sector reforms have affected market dynamics and the real economy, we need to go a step further. In 2017, the G20 leaders have thus adopted a framework for the evaluation of reform effects (FSB 2017). The framework has three elements: causally attributing observed trends in financial markets to reforms, understanding relevant heterogeneities across banks, countries, and over time, and understanding general equilibrium effects.
One crucial objective of policy evaluation is to make a distinction between the effects of reforms on individual firms, and the effects on the financial system. (Short-term) costs of reforms for the private sector, such as costs of compliance, are often more visible and easier to measure than longer-terms benefits for society as a whole. Some of these benefits arise through shifts in costs from the public to the private sector: withdrawing implicit public guarantees for financial institutions increases the costs of debt, because creditors cannot rely on bailouts and thus demand higher risk premia. This will show up as a private cost. Yet, society as a whole benefits if the financial system becomes more stable and if risk is not shifted to the public sector. 
To see why structured policy evaluation is needed, take the effects of increased capital requirements. Better capitalised banks are more resilient and can lend more to the real economy. At the same time, post-reform trends in bank lending have differed across countries, banks, and industries. This raises a number of questions: Has lending declined because of weaker credit demand? Do banks lend more prudently? What has been the impact of regulations on stability and growth? Does bank lending support sectoral reallocation – and thus growth? Aggregate time trends in lending do not answer these questions. Rather, micro data and empirical methods for (causal) identification are the basis for well-informed policy decisions.
A good data strategy is crucial for policy evaluation. Data on trade repositories have, for example, been an important element of a recent evaluation of derivatives market reforms. The Financial Stability Board (FSB) recently released a consultative report on the impact of post-crisis reforms on incentives for central clearing. Many of the insights in this report are based on trade repository data, but the report also mentions important shortcomings in the use and availability of data.
In 2017, the IFC conducted a survey to investigate data collected by trade repositories. The report, which will be released soon, contains interesting insights:
* Data from trade repositories are useful for macroprudential risk assessment, for analysing market transparency, and for microprudential risk assessment.
* Most jurisdictions collect some data on derivatives, but there are still considerable gaps.
* Using these data for policy work remains limited due to incomplete coverage, reporting lags, and the complexity of the existing data.
The arrival of new technologies and data sources has been another structural change. New data sources – typically summarised under the label "Big Data" – can provide information that complements or even replace traditional statistics. New technologies such as machine learning can allow making better use of existing data and improve the efficiency of producing statistics. Let me briefly review the key trends as they are reflected in the work of the IFC.
Big Data has become a buzzword. It can open up new business opportunities and information sources, but it also raises concern about data confidentiality and privacy. In 2015, the IFC thus reached out to central banks to learn about the relevance of Big Data to central banks. Big data are flexible and available in real time. They thus complement existing statistics and support policy analysis. Yet central banks’ actual involvement has remained limited, given the costs of handling and using Big Data. The IFC has decided to start pilot projects, related not only to the internet but also to the various large micro datasets that are already available (eg administrative, commercial and financial market datasets). Conferences and workshop events in this area have been co-organised in 2017 and 2018 with the Bank of Indonesia.
Another area in which technological advances are delivering new opportunities for central bankers and statisticians is the realm of machine learning. For example, the Bundesbank currently tests a neuronal network for generating forecasts on price movements in US-Treasuries over short time horizons (1-10 days) in order to support and inform the decision making process in portfolio management. Other projects explore the use of machine learning for quality assurance, record linkages, or merging datasets. The IFC continues to support cross-institutional collaboration in this space through conferences such as this one.
The central banking world is also increasingly engaged in understanding the impact of FinTech on the financial system and financial stability. FinTech can be defined as technologically-enabled financial innovation that could result in new business models, applications, processes or products with an associated material effect on the provision of financial services. It is rapidly modifying the structure of financial markets, by eg fostering new forms of credit (peer-to-peer lending), leading to the emergence of cryptocurrencies, and prompting changes in payments systems. The rapid innovation of FinTech means risks are constantly changing and there are many avenues to explore.
In response to these developments, the IFC has committed to undertaking further work in this area to support central banks’ understanding of risks and opportunities related to FinTech. Yesterday, the IFC Committee agreed to set up a task force to work on data issues related to FinTech, in close cooperation with international standard setters including the Basel Committee on Banking Supervision (BCBS) and the Committee on Payments and Market Infrastructures (CPMI).
A lot has happened in the ten years that have elapsed since the crisis: Data gaps have been closed; new technologies transform the production of statistics; structured evaluations of post-crisis financial sector reforms have been kicked off at the G20 level.
In order to sustain these efforts and to transform "data" into "information", let me emphasise a few points:
First, central bank statistics are used by many stakeholders: analysts, researchers, policymakers, and private market participants. These stakeholders will use our statistics only if they are easily accessible. Communication with stakeholders, a clear legal basis to support data sharing, the use of joint methodological standards, and measures to protect confidential information are thus crucial (IFC 2015).
Second, when discussing new data initiatives, the costs and benefits need to be weighed up carefully. Detailed, granular data improve analytical work, and thus contribute to better policies and public welfare. But reporting of statistical and supervisory information can be costly, in particular if data initiatives are not well coordinated. Improving the trade-off between costs and benefits requires long-term strategic planning and consultation with industry. 
Third, policy analysis and statistical work can be integrated and coordinated more closely. Before launching policy projects, data needs should be carefully considered, and sufficient time should be budgeted for data work. Statistical initiatives have to take into consideration the type and granularity of data needed to conduct good causal evaluation studies. Both require close coordination between statistical and policy departments in central banks.
