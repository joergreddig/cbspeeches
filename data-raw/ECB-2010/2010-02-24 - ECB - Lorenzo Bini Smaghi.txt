“The economist John Maynard Keynes is back in fashion”, writes Robert Skidelsky in the preface of his most recent book ‘Keynes, the Return of the Master’, published less than one year ago. The reason, says Skidelsky, is not so much that the world is facing the worst crisis since the Great Depression and that governments all over the world have embarked on stimulative packages to support the economy, as the author of the General Theory had advocated, but because Keynes “provides the right kind of theory”, a theory which is “ an indispensable guide for the future”.
A question that comes to mind in reading these words is why Keynes had to make a comeback in the first place, why the General Theory was forgotten and its prescriptions abandoned. If the Theory was indeed general, it should apply in all circumstances and not only when the world is on the verge of collapse.
There are two possible answers to the question. The first is that the Theory is not general, and thus cannot apply to all economic states of the world. The second is that Keynes’ famous admonition in the last page of his General Theory – according to which “Practical men, who believe themselves to be quite exempt from any intellectual influences, are usually the slaves of some defunct economist” – has been turned upside down. In other words, defunct economists – and their theories – are usually enslaved by practical men who do not fully understand them.
I side with the second interpretation and will use this lecture today to explain why. In my view, the most important contribution that Keynes has made throughout his life is to warn practical men, including policy-makers, not to remain trapped in pre-conceived ideas when addressing new issues. In ‘The Economic Consequences of the Peace’ (1919) he warned against adopting the traditional practice of peace treaties – namely, extracting huge reparations from the defeated countries, without taking into account the general economic situation and the burden that this would also impose on the victorious countries. In ‘A Tract on Monetary Reform’ (1923), he warned against a precipitous return to the Gold Standard which prevailed before the war as a way to reinstate monetary discipline and fight against inflation. In ‘The General Theory of Employment, Interest and Money’ (1936) he warned against the predictions of the classical theory, in particular when the underlying assumptions of such a theory are not fulfilled, as was the case during the Great Depression.
Throughout his life Keynes questioned mental constructions based on very restrictive assumptions. He shows the limitations of such approaches in ‘The General Theory’ by putting at the centre of his analysis the role played by uncertainty in shaping economic outcomes. As he pointed out: “The state of confidence, as they term it, is a matter to which practical men always pay the closest attention. But economists have not analysed it carefully.” In his view, the pervasive role played by uncertainty in the economy explains a number of otherwise puzzling departures of the real world from the idealised system described by classical general equilibrium theory. In particular, it explains why a state of depressed expectations can persist, and can lead to a protracted recession, or even to a depression. According to Keynes, the ‘state of confidence’ is constantly in flux. When it is high, business thrives; when it is depressed, on the other hand, the economy contracts, and can even end up in a permanent state of under-utilisation of resources.
Keynes’ analysis of the need for and role of government intervention is predicated on the state of uncertainty prevailing in the economy. The main contribution that economic policies can make is to reduce uncertainty. Policies are not intrinsically ‘good’ or ‘bad’, but rather, their effectiveness depends, first, on the extent to which they decrease or increase macroeconomic uncertainty, and second, on their ability to coordinate agents’ expectations towards a better equilibrium. Keynes was not in favour of systematic government intervention in the economy under all circumstances and all states of the world. In his view government should abstain from interfering with the normal functioning of the economy and should only step in under exceptional circumstances, in particular in the presence of a heightened state of uncertainty. This point was unfortunately lost on many of his followers after the Second World War, who advocated government intervention under all circumstances and all states of uncertainty, thus leading to policy mistakes such as the Great Inflation of the 1970s.
To make sure that Keynes is back for good, and will not be returned to the shelves when the crisis is over, we need to fully understand the breadth of his analysis, in particular the role played by uncertainty in shaping economic outcomes. I would like to make a contribution here by reviewing a few key aspects of his analytical work regarding the importance of uncertainty, the role of government intervention and the effectiveness of stabilisation policies.
 In his Treatise on Probability, Keynes discussed three types of uncertainty. The first, and easiest to characterise and handle within macroeconomic models, is cardinal uncertainty. Its key feature is that it can be measured, and that a specific number can be attached to it. For example, we can say that, based on data from the last (say) thirty years, there is an X percent probability that next year the euro area economy will grow by Y percent. An important assumption is that the structure of the economy must remain sufficiently stable over time, or, in case it experiences time-variation, both the type of variation, and its pace (or velocity) must be known with a reasonable degree of accuracy.
The second type is ordinal uncertainty. In such a situation, although no specific number can be attached to the probability of a specific event, we are still in a position to state that event A is more likely than event B. According to Keynes this was by far the single most important probability class. It’s important to stress that this second type of uncertainty lies outside the dominant macroeconomic framework. Indeed, according to the rational expectations hypothesis, agents use the structure of the model in order to compute objective, numerical probabilities of alternative events occurring. Even in models where expectations are not fully rational, but rather computed on the basis of learning schemes, probabilities are computed econometrically.
Finally, there is the so-called ‘irreducible uncertainty’ – a concept originally introduced by Frank Knight – which is simply based on the fact that agents have no rational basis for making any probabilistic statement of any kind about a specific event occurring or not occurring.
It is obviously very difficult to identify which type of uncertainty prevails at a specific point in time, especially with respect to this third one.
There is ample evidence to suggest that the 1930s was a period in which the third type of uncertainty dominated. Paul Valéry, writing in 1931, captured the zeitgeist well: “Instead of, as in the past, playing an honest card game with fate, knowing the rules, knowing the number of cards and face cards, we are now in the position of a player who is astonished to discover that his adversary’s hand contains face cards never seen before, and that the rules of the game change with each hand.” As The Magazine of Wall Street put it a month after Black Thursday : “Uncertainty is worse than knowing the truth, no matter how bad”. This was the psychological climate and state of mind Franklin Roosevelt was referring to when, in his inaugural address as President of the United States in 1933, he stated that “the only thing we have to fear is fear itself”.
The economic literature confirms the uncertainty prevailing during those days. In their classic Monetary History of the United States, Friedman and Schwartz pointed out that: “The contraction after 1929 clearly shattered beliefs in a “new era”, in the likelihood of long-continued stability […]. The contraction instilled instead an exaggerated fear of continued economic instability, of the danger of stagnation, of the possibility of recurrent unemployment”. Events such as the 1929 stock market crash, or the September 2008 collapse of Lehman Brothers, create a vast amount of ‘irreducible uncertainty’, which exerts paralysing effects on spending decisions and may cause the economy to slide into a depression.
During the months immediately following the 1929 crash, indeed, both consumer purchases of durable goods and business investment dropped sharply, while spending on perishable goods rose slightly. Christina Romer (1990) argues that the most likely explanation for this phenomenon is that the uncertainty generated by the financial crisis induced both consumers and firms to put off purchases of durable and investment goods, respectively, thus causing the economy to stall and contract. The mechanism underlying this ‘wait and see’ attitude during periods of heightened uncertainty has been well known since the work of Bernanke (1983), and it has been extensively explored by Dixit and Pindyck (1994). The key point is that when a specific purchase is irreversible, as is the case with either investment or durable goods purchases, agents must make a trade-off between the benefit deriving from an early commitment and those deriving from waiting for better information. Within such an environment, an increase in uncertainty can therefore naturally lead to a postponement of expenditure decisions in order to get better information, thus causing a fall in demand.
The effects of irreducible uncertainty tend to have a long-lasting economic impact. For instance, one of the leading explanations for Mehra and Prescott’s (1985) ‘equity premium puzzle’ – i.e. the large excess return of stocks over the risk-free rate which has characterised the last several decades – has to do with the trauma of the Great Depression, and with the public’s gradual and slow realisation of the aberrant nature of those years, which led the equity premium to gradually decrease from the peaks reached during the 1930s. 
 Classical theory maintains that government should not interfere with the interaction between economic agents, because this would lead to a sub-optimal allocation of resources. This approach may apply also to situations of heightened uncertainty, to the extent that sooner or later the economy is expected to get back to normal. By not intervening, the policy-maker avoids introducing distortions into the economy which may cause further and graver problems later on.
The notion that the excesses and imbalances that lead to a crisis should be allowed to play out and unwind without government intervention was the intellectual rationale behind the position of Herbert Hoover’s Secretary of the Treasury, Andrew Mellon, during the early years of the Depression, when he famously stated: “Liquidate labor, liquidate stocks, liquidate the farmers, liquidate real estate[…]. Purge the rottenness out of the system.” Expressed less dramatically, Mellon’s idea was that the crisis originated in a series of distortions which had been building up in the American economy during the ‘Roaring Twenties’, first and foremost a highly overvalued stock market, and, to a slightly lesser extent, a housing boom. Given that the problem resulted from massive distortions and imbalances that developed over the years, the only way to solve the problem was to wait patiently until the imbalances unwound. Once the storm had passed, the economy would return to equilibrium, an equilibrium, it is important to note, free from the previous distortions.
There are (at least) three reasons why this approach, although at first sight intellectually attractive, might be flawed. The first reason is the theoretical possibility that the economy possesses multiple equilibria, so that an especially large negative shock may push it into a permanent underemployment equilibrium. Over the last three decades a branch of macroeconomic research has further explored the idea, proposed by Keynes in ‘The General Theory’, that the economy may possess a continuum of equilibria, so that autonomous fluctuations in the degree of ‘optimism’ or ‘pessimism’ can move it from one equilibrium to the other, thus generating booms and recessions. Diamond (1982), for instance, develops a model characterised by trading externalities. The key intuition is that a firm’s optimal level of production crucially depends on the probability of finding buyers, which in turn depends on the level of aggregate demand. So if the firm thinks that the probability of selling its output is low, it will produce less. But since this holds for all firms, this attitude will generate a low level of aggregate demand, so that the initial pessimistic belief will be validated. Such an economy exhibits multiple equilibria, and, in principle, there is no reason why one of them should be ‘preferred’, or should be regarded as ‘more likely’ than any other, so that the economy, once dislocated from a high-employment equilibrium by (say) a large bout of pessimism, can potentially remain in the low equilibrium. In recent years, this position has been mainly associated with the work of Roger Farmer, who has produced several examples of economies characterised by a continuum of equilibria. Keynesian ‘animal spirits’ may select one of these as ‘the’ equilibrium which will ultimately prevail. 
The possible existence of multiple equilibria, with a significant fraction of them characterised by extensive unemployment and under-utilisation of productive resources, provides a conceptual rationale for government intervention in the economy. In this kind of environment, government’s fundamental role is to help coordinate the private sector’s expectations in respect of one of the high-employment equilibria. The second reason why government intervention is justified in the face of uncertainty is the possibility of deflation. On this I will be brief, because the argument is well known, and it has been well understood at least since the work of Irving Fisher. The key point is that, because of the zero lower bound on nominal interest rates, once the economy has fallen into a deflationary trap, traditional monetary policy tools, and in particular, changes in the short-term policy rate, are ineffective at lifting it out of the doldrums. To be sure, this does not imply that monetary policy, as more generally conceived, is ineffective under these circumstances. 
The third reason has to do with the fact that the severity of the economic contraction may change the economy’s equilibrium level – or potential output – bringing it down together with actual output. This is routinely ignored by standard macroeconomic models, which are ‘log-linearised’ (in plain English, approximated) around a fixedequilibrium, and for which macroeconomic fluctuations are, by construction, temporary. Although the assumption that the economy’s equilibrium is independent of past business fluctuations represents a useful simplification when dealing with comparatively small fluctuations, this is not the case when we are dealing with collapses on the scale of the Great Depression, for the simple reason that they may trigger chains of events that can lower potential output permanently.
There are several mechanisms through which a comparatively large economic downturn may lead to a permanent decrease in potential output. One mechanism involves the impact of long periods of unemployment on workers’ skill sets, their productivity and their very attachment to the labour force. To put it simply, the longer a worker is unemployed, the more his or her skill set deteriorates because of lack of use, and the more tenuous his or her attachment to the labour force becomes, with the result that, as time passes, the less employable the worker becomes. In the end, some workers even drop out of the labour force altogether. This phenomenon used to characterise Europe, whereas the US labour market, significantly more flexible and dynamic, was essentially immune to it, with a comparatively low fraction of long-term unemployed. It is a measure of the depth of the recession in the United States that, in October of last year, nearly 40 percent of the unemployed had been out of a job for at least six months, whereas 20 percent had been jobless for at least a year. In both cases these were significantly higher figures than the corresponding ones for the recession of the early 1980s.
 Given the previously mentioned problems associated with a classical, laissez-faire approach in the presence of a heightened state of uncertainty, government has the responsibility under such circumstances to intervene in order to counteract the potentially devastating effects on the economy. Since – as previously mentioned with reference to the period following the stock market collapse of 1929 – situations characterised by a heightened state of uncertainty are also typically associated with a collapse in aggregate demand, the effectiveness of stabilisation policies under these circumstances crucially depends on their ability to stimulate aggregate expenditure.
A substantial body of literature has developed in recent decades showing how monetary and fiscal policies can contribute to stabilising income. The literature is based on several alternative approaches, from the so-called ‘narrative approach’ originally introduced by Milton Friedman and Anna Schwartz in their historical analysis of US monetary policy, to the technically clearest evidence coming from structural vector autoregressions and estimated general equilibrium macroeconomic models. 
Macroeconomic policies can contribute to decrease heightened uncertainty by supporting aggregate demand but also by improving the functioning of markets. As we have seen recently, a key characteristic of such periods is that financial markets’ normal trading activity tends to become severely impaired, to the point that a significant fraction of operators may even stop trading with one another, thus leading to a credit crunch, and to a further contraction in aggregate demand. Under these circumstances, central banks can intervene, for example, by accommodating banks’ liquidity demand at a pre-specified interest rate, lengthening the maturities of long-term refinancing operations, or expanding the list of assets which banks can use as collateral. All these measures support the functioning of financial markets, thus countering the effects of the credit crunch and ultimately decreasing irreducible uncertainty and supporting aggregate demand. This prevents the economy from entering a recessionary loop, in which the freezing-up of financial markets leads to a contraction of credit, which, in turn, leads to an economic contraction, additional uncertainty, and further financial market immobility, in a self-reinforcing cycle.
The fact that macroeconomic policies can help to stabilise the economy, and prevent deep downturns, by reducing the heightened state of uncertainty prevailing among economic agents does not mean that they are always effective, under all circumstances. On the contrary, experience has shown that these policies may well turn out to be destabilising, especially if they are implemented in a way which increases the state of uncertainty, instead of reducing it. This may happen, in particular, if the size of the fiscal and/or monetary injection is such that irreducible uncertainty is increased, rather than reduced.
A case in point is when a government runs up a succession of very large deficits and ends up accumulating an excessive stock of national debt. At the limit, this may even lead to a dislocation of inflation expectations, as financial markets start to expect that part of the national debt will be simply ‘inflated away’ – that is, it will be eroded viahigher inflation – with a resulting increase in interest rates. By definition, preventing a dangerous build-up of national debt requires keeping budget deficits under control. When this is especially difficult because of the sheer magnitude of the recession, governments should – at the very minimum – provide financial markets with a clear ‘re-entry plan’, carefully specifying both the timeframe within which the deficit will be brought under control, and the specific measures the particular government intends to take.
The need to avoid creating excessive uncertainty about a country’s long-term fiscal solvency represents the fundamental limit to the extent of the fiscal stimulus which can be injected into the economy. As Skidelsky observes, Keynes himself would not have excluded balanced budgets even during a recession, if this was needed to minimise the risk of sowing doubts about long-term fiscal solvency in the minds of market operators. The alternative is an increase in the risk premia that financial markets require in order to absorb government bonds, and therefore an increase in interest rates. In turn, this results in lower capital accumulation, and therefore, ceteris paribus, in lower productivity and output per capita.
Empirical evidence suggests that countries whose level of national debt exceeds a certain threshold are also characterised by comparatively lower growth: Reinhart and Rogoff (2010), analysing a dataset which spans the last few centuries and comprises several dozen countries, show that, for those countries with a government debt-to-GDP ratio of over 90 percent, median GDP growth tends to be about one percentage point lower than it would be otherwise. Although at first sight this may appear small, when cumulated over decades it leads to significantly lower standards of living.
Empirical evidence also shows that higher deficit-to-GDP and debt-to-GDP ratios lead to higher interest rates even when there are no doubts about fiscal solvency. A study by Thomas Laubach based on US data, for example, estimates that every percentage point increase in the projected deficit-to-GDP ratio results in an increase in long-term interest rates of 20 to 29 basis points, whereas a percentage point increase in the debt-to-GDP ratio is associated with an increase of about 3 to 4 basis points. Once again, although such effects may seem tiny, when cumulated over long periods of time their impact becomes significant.
Another fundamental problem of fiscal policy is its lack of timeliness, or, to put it differently, its implementation lags. Whereas automatic stabilisers such as unemployment benefits are built into the system, and therefore have no implementation lags to speak of, decisions concerning taxation and expenditure are legislated by parliament. As a result, as extensively discussed in standard macroeconomic textbooks, a fiscal stimulus may come to be implemented with a potentially significant delay. A conceptually related problem with using fiscal policy as a stabilisation tool is that its need to ‘pass through the political process’ inevitably decreases its predictability. Fiscal stimulus packages, quite inevitably, become part of a larger bargaining process between competing interest groups, so that not only the timing, but also the extent and composition of the fiscal stimulus become comparatively uncertain.
Monetary policy has also its limitations when it comes to stabilisation purposes. One problem is that low interest rates, if protracted, may fuel new imbalances, in particular bubbles in asset markets, increasing uncertainty across the board, rather than decreasing it. Once again, this is nothing but an illustration of the same general point I previously made with reference to the need to guarantee fiscal solvency: the fundamental limit to government stabilisation policies is that such interventions should never increase overall uncertainty, and they should rather decrease it.
This point was very clear to Keynes, but it was unfortunately lost on many of his followers during the early decades of the post-WWII period, who advocated government intervention under all circumstances, in order to keep the economy as close to potential as possible – that is, to ‘fine-tune’ it. The dangers intrinsic to such an approach to stabilisation policy were pointed out by Federal Reserve Chairman William Martin in his February 1965 testimony before the Joint Economic Committee of the US Congress when he stated that: “There is, inevitably, an element of brinkmanship in our laudable efforts to push our economy closer and closer to its full potential without straining it”. The ‘brinkmanship’ Martin mentioned has to do with a key limitation of ‘fine-tuning’ policies which had long been discussed by monetarists such as Milton Friedman and Karl Brunner: the use of stabilisation policies under all circumstances is predicated on a reasonably precise knowledge of equilibrium levels, such as potential output and the natural rate of unemployment. Since they are both unobserved, and they both change over time, the use of such policies under normal circumstances, in order to keep the economy constantly close to potential, is inevitably bound to fail. And indeed, one of the dominant explanations for the Great Inflation of the 1970s in the US traces its origins to the combination of ‘fine-tuning’ policies and the inability of the Federal Reserve to detect the productivity slowdown of the 1960s-1970s in real time, thus resulting in a systematic over-estimation of the true extent of slack in the economy. Since output gap measures were a key ingredient in the Fed’s monetary policy process, this automatically resulted in an excessively loose monetary policy, thus igniting and then perpetuating the Great Inflation.
The fact that the origin of the inflationary upsurge of the 1970s is to be found in the fine-tuning policies of that era provides a stark illustration of the dangers associated with systematic government intervention under normal circumstances – specifically, under circumstances which are not characterised by irreducible uncertainty. As extensively discussed by Milton Friedman in his critique of so-called “Keynesian stabilisation policies”, even in a best-case scenario the systematic use of such policies injects noise into the economy, thus increasing uncertainty across the board. This was the key rationale for Friedman’s rejection of discretion in favour of a simple monetary policy rule such as the ‘k-percent rule’, according to which the central bank should focus on stabilising the rate of growth of the money supply at a specific value.
There is a further, and subtler aspect to this. As was demonstrated by the rational expectations school back in the 1970s, if agents are rational, only the unsystematic component of policy – that is, only a policy surprise – has an impact on macroeconomic outcomes. But this implies that a policy-maker who aims to pursue stabilisation policies can only do that by systematically surprising the private sector, which is an obvious contradiction in terms, as policy surprises, by definition, cannot be systematic: if they were systematic, indeed, they would be taken into account by private agents when forming expectations, and they would therefore be nullified. The conclusion, once again, is that, unless the economy is in a state of extreme uncertainty, systematic government intervention injects noise into the system, therefore increasing overall uncertainty.
To be sure, this does not imply that any kind of government intervention per se is counterproductive: structural policies aimed at (e.g.) fostering productivity growth, or increasing labour force participation rates, are examples of beneficial government interventions. The key reason why such policies are beneficial, however, is that rather than attempting to stabilise the economy around an unobserved and imprecisely estimated equilibrium level, they aim to increase such a level. As a consequence, structural policies do not suffer from the drawbacks afflicting systematic stabilisation policies under normal circumstances.
 Keynes’ major contribution has been to challenge common wisdom, which rests on human beings’ innate tendency to tackle the problems they face in the same way as they did in the past, expecting to achieve the same outcome, even when circumstances differ. After the First World War, he accused the leaders of the time of “attacking the problems of the post-war world with unmodified pre-war views and ideas”. He repeated this charge after the Great Depression. Their failure to listen led to even greater problems.
What are the lessons for our critical times?
Let me elaborate on my personal views, considering in particular the role of policy-makers. Keynes has taught us to assess carefully the state of the world we live in before designing the policies that authorities should implement. When heightened uncertainty prevails, government has a role to play in adopting measures that contribute to reducing the state of uncertainty and in helping economic agents coordinate expectations towards a socially optimal outcome. This raises a few difficulties for policy authorities. First, they have to assess whether the state of uncertainty is such as to justify active intervention. Second, they have to design policies that help to reduce uncertainty, rather than increase it. History is full of examples of policy mistakes made in both directions, i.e. of states of heightened uncertainty which were not recognised and were not accompanied by adequate actions, and states which were considered as being of heightened uncertainty even though they were not and were accompanied by policy actions which increased uncertainty instead of decreasing it. These are the typical type I and type II errors recognised in statistical analysis.
How can we avoid making these mistakes, in the midst of the deepest economic slump since the Second World War? In particular, how can we avoid the temptation to address the problems of the post-crisis world with unmodified pre-crisis views and ideas?
At the start of the crisis, authorities around the world intervened massively, using both monetary and fiscal instruments. This prevented a total collapse of economic activity, which has now started to recover. The type I error has been avoided. Keynes’ lesson has been understood.
The key question, from now on, is how to avoid the type II error, which consists of overstating the conditions of uncertainty and implementing policies which may increase uncertainty, rather than decrease it. I see here a risk that the lessons of the crisis will be quickly forgotten if the analytical framework underlying the policy decisions is not substantially improved. Let me give some examples.
First, most models continue to assume that markets price assets in an efficient way and that agents form expectations in a fully rational way. The state of uncertainty in the economy continues to be assessed on the basis of market indicators, which have proven in the recent past to be potentially quite misleading. For example, the sustainability of monetary and economic policies is often evaluated in accordance with market-based indicators related in particular to inflation expectations and credit risk premia. Low inflation expectations and default premia are often interpreted as a sign that markets are attributing a low probability to the risk that policies may become unsustainable. However, we have seen in the recent past that markets tend to adjust prices in a discontinuous way. Analysing policy sustainability exclusively on the basis of market indicators may create the illusion that inflation expectations are well anchored or that markets are willing to continue finance large budget deficits, thus creating an incentive for unduly prolonging very expansionary policies.
Second, most models continue to be based on an unchanged belief about the capacity of monetary and fiscal policy action to pilot the economy towards full employment. As mentioned previously, Keynes taught us that policy multipliers change depending on the underlying state of the economy and on its prospects. This insight was rationalised in the mid-1970s by Robert Lucas, a neo-classical economist, in his seminal critique of econometric policy evaluation. In spite of this rare conjunction of minds, mainstream analytical frameworks, ironically named neo-Keynesian DSGE, largely ignore this aspect. No wonder that the policy analysis based on these models tend to be backward looking. For example, some recent suggestions have gone to the extreme of advising monetary policy to target a higher rate of inflation than what central banks currently do, so as to create more room for manoeuvre for monetary policy to react when bubbles burst without prematurely hitting the zero lower bound for interest rates. It’s as if to say that the main problem with monetary policy in the recent past was that the so-called Greenspan put had not been potent enough to avoid the negative consequences of the bursting of the bubble, rather than being a mistaken policy which fuelled the bubble in the first place. “ Errare humanum est, perseverare diabolicum!”
Third, and most important, analytical models that underpin policy decisions are based on a key variable which characterises the long-term steady state of the economy. Recent debates have focused on whether and how the growth potential of our economies will be affected by the current crisis. This is obviously the key to understanding the current state of slack in the economy, the size of the output gap and thus the degree of stimulus which is required to push the economy towards its full employment level. But it seems to me that the debate on this issue is based on the wrong assumption. It is generally agreed that in the years preceding the crisis economic activity was on an unsustainable path because of inappropriate macroeconomic, regulatory and supervisory policies which particularly encouraged excessive debt accumulation. It is thus considered that changes in these policies would make it possible to avoid repeating the mistakes of the past although, as I just showed, there is no guarantee that such wisdom will prevail. What is missing, in my view, is some deeper thinking about why policy-makers and economic agents more generally took inappropriate decisions.
I believe the actions and policies were unsustainable because they were aiming at an objective which was not sustainable. At the heart of the wrong decisions that were made in the past, and which led to the crisis, was the expectation that the increase in prosperity experienced by our societies in previous decades would simply continue, unaffected by the ongoing integration of emerging market countries into the global system. Policies, be they regulatory or macroeconomic, aimed at achieving a rate of economic growth which was based on past experience and which turned out to be too high. These policies were thus too expansionary, leading agents to borrow excessively, and they fuelled a bubble which ultimately burst. In my view, advanced economies have not appropriately taken into account how globalisation, and the integration of hundreds of millions of people into the market economy, might affect their standard of living, including the possibility of a slower pace of economic growth and greater inequalities within societies. A few years ago Samuelson dared to challenge the common wisdom according to which the law of comparative advantage always and everywhere benefits all parties engaging in trade. His hypothesis implied that a fast rate of growth in some parts of the world economy could reduce the rate of growth, and increase its dispersion, in other parts of the world. His ideas were rapidly dismissed, not only in academic circles but more generally by society. There is, to my knowledge, very little research being done in this field, with the notable exception of Ragu Rajan. 
I think that Samuelson’s analysis can help to explain why the trend rate of income growth in the advanced economies might have slowed down substantially during the last few years, upsetting the social and political balance on which that growth rested. In particular, it helps to understand why, for the first time since World War II the prospects for the younger generations appear to be overall less promising than for the previous ones. And why advanced economies, by pursuing policies with an ultimately unrealistic objective, fell into a crisis.
If the above hypothesis is correct, then it’s a mistake to think that we can get out of this crisis and back on a path of sustainable growth only through the support of fiscal and monetary policies. To paraphrase Keynes, monetary and fiscal policies are “a subtle device for linking the present to the future”. These policies may be seriously misguided if they are based on false expectations about the future. On the other hand, the success of these policies in averting a collapse of the world economy right after the Lehman Brothers’ failure, in the autumn of 2008, might fuel the illusion that the same policies can bring us back to the halcyon days of mid-2007, when we were “still dancing” as one famous banker put it. It would be an illusion, as I just said. And a waste of time and effort.
To avoid this waste, we need to come back to Keynes’ intuition, which is that the problems of the future cannot be dealt with simply by applying the solutions of the past, because the problems of the future are different from the problems of the past. If we want to put our societies back on a path of sustainable prosperity we must stand ready to fundamentally reform the way in which our economies work and compete in the global environment.
In doing that, we may well have to resuscitate some defunct economist. And read their work carefully.
